{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQxydkwLSsxo"
   },
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq7cUpmVSqoP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to read data\n",
    "import requests\n",
    "\n",
    "# for neural network\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# to save parameters\n",
    "import copy\n",
    "\n",
    "# to plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for splitting data and evaluating results\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# for filtering and normalization\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to track progress\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Larwq0JQS67g"
   },
   "source": [
    "## function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsno8AswTivy"
   },
   "source": [
    "### download file from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7-ia5OVTETH"
   },
   "outputs": [],
   "source": [
    "def download_file(url,saveAs):\n",
    "    if not os.path.exists('anamoly_dataset.pickle'):\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(saveAs, 'wb').write(r.content)\n",
    "        print('file downloaded')\n",
    "    else:\n",
    "        print('file already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyLWnj3zToIm"
   },
   "source": [
    "### create patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFubO545T0il"
   },
   "outputs": [],
   "source": [
    "def interpolate(arr,newSize):\n",
    "  l = len(arr)\n",
    "  indices = list(range(0,l))\n",
    "  newIndices = np.linspace(0, l-1 , newSize)\n",
    "  newArr = [np.interp(i,indices,arr) for i in newIndices]\n",
    "  return newArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocX8LNXhTr2N"
   },
   "outputs": [],
   "source": [
    "# function to return list of patches for a given dataset\n",
    "def dataToPatches(data, window_size, stride, resizeTo, smooth=False, normalize=False):\n",
    "  inputs = []\n",
    "  labels = []\n",
    "    \n",
    "  for sample in tqdm(data):\n",
    "    input = sample[0]\n",
    "    label = sample[1]\n",
    "\n",
    "    # get length excluding nan values which indicate end of input\n",
    "    lens = []\n",
    "    for channel in input:\n",
    "      channel = list(channel)\n",
    "      len1 = np.where(np.isnan(list(channel)))[0][0] if any(np.isnan(channel)) else len(channel)\n",
    "      lens.append(len1)\n",
    "    inputLen = np.min(lens)\n",
    "\n",
    "    for i in range(0, inputLen, stride):\n",
    "      channels = []\n",
    "      # verify if last stride is possible\n",
    "      if i + window_size in range(inputLen + 1):\n",
    "        for channel in input:\n",
    "          values  = [0]*i\n",
    "          values += list(channel)[i:i+window_size]\n",
    "          values += [0]*(inputLen - i - window_size)\n",
    "          values = interpolate(values,resizeTo)\n",
    "          # apply gaussian filter for smoothing and reducing noise\n",
    "          if smooth:\n",
    "              values = gaussian_filter(values, sigma=1)\n",
    "\n",
    "          indicator  = [0]*i\n",
    "          indicator += [1]*window_size\n",
    "          indicator += [0]*(inputLen - i - window_size)\n",
    "          channels.append(values)\n",
    "          # Normalize between 0 and 1\n",
    "          if normalize:\n",
    "              channels = np.array(channels)\n",
    "              shape = channels.shape\n",
    "              channels = list(MinMaxScaler().fit_transform(np.array(channels).reshape(-1,1)).reshape(shape))\n",
    "\n",
    "        indicator = interpolate(indicator,resizeTo)\n",
    "        channels.append(indicator)\n",
    "\n",
    "        inputs.append(channels)\n",
    "        labels.append(label)\n",
    "        \n",
    "  inputs, labels = np.array(inputs), np.array(labels, dtype=int)\n",
    "  return inputs,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWCtFPBUEwM"
   },
   "source": [
    "### create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWkOFtnkUQG1"
   },
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "  def __init__(self, inputs, labels):\n",
    "    self.inputs = inputs\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    input = self.inputs[index]\n",
    "    label = self.labels[index]\n",
    "    return input,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jraK9cDkULT5"
   },
   "outputs": [],
   "source": [
    "# function to create train, val and test loaders\n",
    "def createLoaders(train_inputs, train_labels, test_inputs, test_labels, batch_size, val_percent=.25):\n",
    "    train_inputs, val_inputs, train_labels, val_labels, = train_test_split(train_inputs, train_labels, test_size=val_percent, random_state=0)\n",
    "\n",
    "    train_dataset = mydataset(train_inputs, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = mydataset(val_inputs, val_labels)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_dataset = mydataset(test_inputs, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader,val_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYPgFHK1UUTP"
   },
   "source": [
    "### train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNjQ0abZUbDT"
   },
   "outputs": [],
   "source": [
    "def trainNet(net,criterion,optimizer,train_loader,val_loader,epochs,print_every=None):\n",
    "    \n",
    "    if not print_every:\n",
    "        print_every = int(epochs / 6)\n",
    "\n",
    "    avg_trainLosses = []\n",
    "    avg_valLosses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        net.train()\n",
    "        for i, (inputBatch,labelBatch) in enumerate(train_loader):\n",
    "\n",
    "            inputBatch, labelBatch = inputBatch.to(device), labelBatch.to(device)\n",
    "            inputBatch = inputBatch.float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputBatch = net(inputBatch)\n",
    "            loss = criterion(outputBatch, labelBatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        net.eval()\n",
    "        for i, (inputBatch,labelBatch) in enumerate(val_loader):\n",
    "          with torch.no_grad():\n",
    "\n",
    "            inputBatch, labelBatch = inputBatch.to(device), labelBatch.to(device)\n",
    "            inputBatch = inputBatch.float()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputBatch = net(inputBatch)\n",
    "            loss = criterion(outputBatch, labelBatch)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "        avg_trainLoss = sum(train_loss) / len(train_loss)\n",
    "        avg_valLoss = sum(val_loss) / len(val_loss)\n",
    "        avg_trainLosses.append(avg_trainLoss)\n",
    "        \n",
    "        if (epoch > 0) and (avg_valLoss < min(avg_valLosses)):\n",
    "            best_params = copy.deepcopy(net.state_dict())\n",
    "            best_epoch, best_loss = epoch, avg_valLoss\n",
    "        avg_valLosses.append(avg_valLoss)\n",
    "\n",
    "        # print statistics\n",
    "        if epoch % print_every == print_every - 1:\n",
    "          print('epoch: %d, train loss: %.3f, val loss: %.3f' % (epoch + 1, avg_trainLoss, avg_valLoss))\n",
    "\n",
    "    print('Finished Training')\n",
    "    plt.plot(avg_trainLosses, label='train loss')\n",
    "    plt.plot(avg_valLosses, label='val loss')\n",
    "    plt.plot([best_loss]*epochs, linestyle='dashed')\n",
    "    plt.plot(best_epoch, best_loss, 'o')\n",
    "    plt.legend()\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdx62XejUlTE"
   },
   "source": [
    "### evaluate and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvzuMsneSifl"
   },
   "outputs": [],
   "source": [
    "def evaluate(net,data_loader,classes):\n",
    "  y_true= []\n",
    "  y_pred = []\n",
    "  net.eval()\n",
    "\n",
    "  for _, (inputBatch,labelBatch) in enumerate(tqdm(data_loader)):\n",
    "    with torch.no_grad():\n",
    "      inputBatch, labelBatch = inputBatch.to(device), labelBatch.to(device)\n",
    "      inputBatch = inputBatch.float()\n",
    "      outputBatch = net(inputBatch)\n",
    "\n",
    "      for output,label in zip(outputBatch,labelBatch):\n",
    "        output, label = output.cpu(), label.cpu()\n",
    "        y_true.append(label)\n",
    "        pred = np.argmax(output)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "  print(classification_report(y_true, y_pred, target_names=classes, labels=range(len(classes)) ,digits=4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "myfunctions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
